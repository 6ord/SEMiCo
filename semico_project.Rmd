---
title: "semico"
author: "Jake Daniels"
date: "October 1, 2018"
output: html_document
---

```{r}
library(tidyverse)
library(rtweet)
library(tidytext)
library(wordcloud2)
data(stop_words)
``` 

```{r}
#insert your own tokens
create_token(app = Sys.getenv("TWITTER_APP"),
  consumer_key = Sys.getenv("TWITTER_CONSUMER_KEY"),
  consumer_secret = Sys.getenv("TWITTER_CONSUMER_SECRET"))
```

WordCloud
```{r}
ryePallete <- c('#1C5F9B', '#C1138A', '#2E1D3B', '#FCC210')

wc <- function(x) {
wordcloud2(x, size=0.7, color=rep_len(ryePallete, nrow(x)))
  save.image(paste0(local_trends[i,]$trend,".png"))
}
```

Top 5 Trends
```{r}
local_trends <- get_trends("toronto") %>%
  arrange(desc(tweet_volume)) %>%
  select(trend) %>%
  head(5) 
```

Getting Trend Datas
```{r}
tweet_funct <- function(x){
search_tweets(x,  n = 2000, include_rts = FALSE,  lang = "en", type="recent", retryonratelimit = F)
}
library(purrr)
lt <- map_dfr(local_trends$trend, tweet_funct, .id	= "trend")
#lt <- map_df(local_trends$trend, search_tweets)

for(i in 1:5) 
lt[which(lt$trend==i),]$trend <- local_trends[i,]$trend
```

Visual #1
```{r}
palettes <- c('Accent', 'Dark2', 'Paired', 'Pastel1', 'Pastel2', 'Set1', 'Set2', 'Set3', 'BrBG', 'PiYG', 'PRGn', 'PuOr', 'RdBu', 'RdGy', 'RdYlBu', 'RdYlGn', 'Spectral')

gg<-function(graph){
my_title <- paste("Online Discussion Surrounding", local_trends[i,]$trend)
my_subtitle <- paste("based on", 2000, "recent tweets")
  ggplot(data=graph,aes(x = reorder(word, n), y = n, fill = "blue")) + 
    geom_bar(show.legend = F, stat = "identity", width = 0.8) +
    scale_fill_brewer(palette = sample(palettes,1)) +
    labs(title = my_title, subtitle = my_subtitle, y = "Number of Mentions", x = NULL) +
    coord_flip() +
    theme_classic() +
    theme(plot.title=element_text(family='', face='bold', size=16))
  ggsave(paste0(local_trends[i,]$trend,".png"), path="/Users/jakedaniels/SEMiCo_proj")
}
```

# create a sequence of final functions to work through, sample(gg,wc,last) ?
```{r}
for(i in 1:5)
print(lt %>%
  filter(trend == local_trends[i,]$trend) %>%
  unnest_tokens(word, text) %>%  
  anti_join(stop_words) %>%
  count(word, sort = T) %>% 
  filter(!word %in% c('t.co', tolower(local_trends[i,]$trend), str_replace(local_trends[i,]$trend, "#", ""), str_extract(pattern = "[a-z]+",tolower(local_trends[i,]$trend)), str_extract(pattern = "[a-z]+$",tolower(local_trends[1,]$trend)), 'https', 'amp', 1:10)) %>%
  arrange(desc(n)) %>%
  head(20) %>%
  gg())

# lemma
# sample(c(gg,wc),1)
```

# CHANGE TOKEN TO SEMICO
```{r}
library(twitteR)
library(httr)
setup_twitter_oauth(consumer_key = Sys.getenv("TWITTER_CONSUMER_KEY"), consumer_secret =Sys.getenv("TWITTER_CONSUMER_SECRET"), access_token = "951665733859233792-RuDfOTRGxagpFfVR3bs3zyRRctbGCEP", access_secret = "qh5ORSTqUMD99ukSRmsXLzMJfDiuLvZuRPkUxd9blooLs")
```

text simulator
```{r}
seq1 <- c(
"Whoa man,",
"Incredible,",
"We noticed",
"Uhhh, any reason",
"Hmm,",
"hmmM,",
"Uhhh",
"Yoooo,",
"!!!! Why ",
"Yeee,",
"haha",
"LOL,",
"whoa damn",
"kinda cool",
"kinda lame",
"pretty stupid",
"pretty silly",
"oddly enough"

)

top1 <- lt %>%
filter(trend == local_trends[i,]$trend) %>%
unnest_tokens(word, text) %>% #Unnest the words - code via Tidy Text
anti_join(stop_words) %>% #remove stop words
count(word, sort = T) %>% #do a word count
filter(!word %in% c('t.co', local_trends[i,]$trend,'https', 'amp', 1:10)) %>%
arrange(desc(n)) %>%
head(1)

seq2 <- c(
"ppl like using these words:",
paste("Seems like they mention",top1,"the most"),
"i'm seeing high volumes of chatter!!!",
paste("i'm detecting lotsa chatter about",top1),
"what is all this chatter about?",
paste("lots of chattering about",top1),
"dang.",
paste("Iunno what",top1,"would even mean"),
"Twitter is blowing up!",
"twitter is all over it",
"drag them kings & queens",
"check please.",
"don't people have better things to do with their time?",
"but i'm hardly even suprised",
"and i'm hardly even suprised",
"but that's no surprise to me",
"what a way to waste my day on lol"
)

marks <- c(rep("?",5), "??", "???", "!?", "?!", "??!", "?!!?", "?!?", "..", ".", "...","....")

sample(paste(seq1,local_trends[i,]$trend,paste0("is trending",marks), seq2),1)
```

```{r}
for(i in 1:5)
tweet(text = sample(paste(seq1,local_trends[i,]$trend,paste0("is trending",marks), seq2),1), mediaPath = paste0("~/semico/", local_trends[i,]$trend, ".png"))
```

```{r}
#top 6 trends - one big pic

bing <- get_sentiments("bing")
sent <- function(x) {
  print(
    x %>%
  unnest_tokens(word, text) %>%  
  inner_join(bing) %>%
  group_by(trend) %>%
  count(word, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
  )
}
local_sentiment <- sent(lt)

ggplot(local_sentiment, aes(index, sentiment,fill=trend)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~trend, ncol = 2, scales = "free_x")
```

```{r}
local_sent_count <- lt %>%
  unnest_tokens(word, text) %>%  
  inner_join(bing) %>%
  anti_join(stop_words) %>%
  group_by(trend) %>%
  count(word, sentiment, sort =T) 

for(i in 1:5)
local_sent_count_1 <- local_sent_count %>%
  filter(n > 25) %>%
  filter(!word %in% c('t.co', tolower(local_trends[i,]$trend), str_replace(local_trends[i,]$trend, "#", ""), str_extract(pattern = "[a-z]+",tolower(local_trends[i,]$trend)), str_extract(pattern = "[a-z]+$",tolower(local_trends[i,]$trend)), 'https', 'amp', 1:10))

local_sent_count_1
  group_by(trend) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  facet_wrap(~trend, scales = "free") +
  geom_col() +
  coord_flip() +
  labs(y = "Contribution to sentiment")

# lemma, #more stop_words, fix edits
```